{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aycaprivate/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import re\n",
    "import string \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "try:\n",
    "     _create_unverified_https_context =     ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "     pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24322, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path_or_buf=\"all.jsonl\", lines=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    '''This function is specifically designed for the HC3 dataset and the preprocessing thereof. \n",
    "    It mostly joins the data, deletes empty rows and duplicate rows.'''\n",
    "    \n",
    "    # Join all data, making it a string instead of a list\n",
    "    df['chatgpt_answers'] = df['chatgpt_answers'].map(lambda x: ''.join(x))\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: ''.join(x))\n",
    "\n",
    "    # Deleting empty rows\n",
    "    df = df.drop(df[df['chatgpt_answers'].map(lambda x: len(x) == 0)].index)\n",
    "    df = df.drop(df[df['human_answers'].map(lambda x: len(x) == 0)].index)\n",
    "\n",
    "    # Deleting duplicate rows\n",
    "    # Answering the same question will result in the same answers\n",
    "    # Therefore focusing on only question column\n",
    "    df = df.drop_duplicates(subset = ['question'], ignore_index = True)\n",
    "    df = df.drop_duplicates(subset = ['human_answers'], ignore_index = True)\n",
    "    df = df.drop_duplicates(subset = ['chatgpt_answers'], ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punc(df):\n",
    "    '''Function to remove punctuation and make everything lowercase.'''\n",
    "        \n",
    "    # Remove punctuation in ChatGPT answers column\n",
    "    df['chatgpt_answers'] = df['chatgpt_answers'].map(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    \n",
    "    # Remove punctuation in human answers column\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: re.sub(r'\\s(\\'\\S)', r'\\1', x))\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: re.sub(r'[^\\w\\s]', ' ', x.lower()))\n",
    "    \n",
    "    # Replace double spaces\n",
    "    df['human_answers'].replace(\"  \", \" \", regex = True, inplace = True)\n",
    "    df['human_answers'].replace(\"   \", \" \", regex = True, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def label_data(df):\n",
    "    '''This functions gives the each answer a label (0 for human answers, 1 for ChatGPT answers).'''\n",
    "    \n",
    "    # Labeling human answers\n",
    "    human = df[['question', 'human_answers', 'source']]\n",
    "    human['type'] = \"human\"\n",
    "    human.rename(columns = {'human_answers':'answer'}, inplace = True)\n",
    "    human['class'] = 0\n",
    "    \n",
    "    # Labeling ChatGPT answers\n",
    "    chatgpt = df[['question', 'chatgpt_answers', 'source']]\n",
    "    chatgpt['type'] = \"chatgpt\"\n",
    "    chatgpt.rename(columns = {'chatgpt_answers':'answer'}, inplace = True)\n",
    "    chatgpt['class'] = 1\n",
    "    \n",
    "    # Shuffle dataset and reset index\n",
    "    df = pd.concat([human, chatgpt], ignore_index = True)\n",
    "    df = shuffle(df)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def data_cleaning(df):\n",
    "    '''This function replaces some of the HC3-specific messiness, mostly in human answers.'''\n",
    "    \n",
    "    df['answer'].replace(\" n t \", \"nt \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" nt \", \"nt \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" s \", \"s \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" ve \", \"ve \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" re \", \"re \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"i m \", \"im \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" ll \", \"ll \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" t \", \"t \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" i d  \", \" id \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" e  \", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"thf_media\", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"url_32\", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"url_40\", \" \", regex = True, inplace = True)\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\r\\n', ' ', x))\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\r\\n\\r', ' ', x))\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\n\\n', ' ', x))\n",
    "    df['answer'].replace(\"  \", \" \", regex = True, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc3_test = data_cleaning(label_data(remove_punc(data_preprocessing(df))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make evenly sized chunks (200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the data into human and chatgpt\n",
    "hc3_human = hc3_test[hc3_test['class'] == 0]\n",
    "hc3_chatgpt = hc3_test[hc3_test['class'] == 1]\n",
    "\n",
    "# Join the data and make it into a list format\n",
    "human_string = ' '.join(hc3_human['answer'].tolist())\n",
    "chatgpt_string = ' '.join(hc3_chatgpt['answer'].tolist())\n",
    "\n",
    "# Split data into words\n",
    "# That way I always have full words\n",
    "human_split = human_string.split(\" \")\n",
    "chatgpt_split = chatgpt_string.split(\" \")\n",
    "\n",
    "# Make evenly sized chunks: 200 words per answer\n",
    "human_list = [human_split[i:i + 200] for i in range(0, len(human_split), 200)]\n",
    "chatgpt_list = [chatgpt_split[i:i + 200] for i in range(0, len(chatgpt_split), 200)]\n",
    "\n",
    "# The chunks are still split\n",
    "# Joining the evenly sized chunks into one \"sentence\"\n",
    "human_joined = []\n",
    "for answer in human_list:\n",
    "    new_list = []\n",
    "    new_list.append(' '.join(answer))\n",
    "    human_joined.append(new_list)\n",
    "\n",
    "chatgpt_joined = []\n",
    "for answer in chatgpt_list:\n",
    "    new_list = []\n",
    "    new_list.append(' '.join(answer))\n",
    "    chatgpt_joined.append(new_list)\n",
    "    \n",
    "# Put both classes into a new Dataframe and concat them together\n",
    "human_df = pd.DataFrame({'answer' : human_joined, 'class' : 0})\n",
    "chatgpt_df = pd.DataFrame({'answer' : chatgpt_joined, 'class' : 1})\n",
    "    \n",
    "# Again, joining the answers, because they were still in a list format\n",
    "human_df['answer'] = human_df['answer'].map(lambda x: ''.join(x))\n",
    "chatgpt_df['answer'] = chatgpt_df['answer'].map(lambda x: ''.join(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Diversity Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611465584771711"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df['ttr'] = human_df['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "human_df['ttr'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.23107570837516"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df['yulek'] = human_df['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "human_df['yulek'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.76497772019333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df['mtld'] = human_df['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))\n",
    "human_df['mtld'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5360142792529885"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_df['ttr'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "chatgpt_df['ttr'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.34450712339435"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_df['yulek'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "chatgpt_df['yulek'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.91089110379055"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_df['mtld'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))\n",
    "chatgpt_df['mtld'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc3_test[['answer', 'class', 'source']]\n",
    "\n",
    "def data_source(df, category):\n",
    "    human = df[df['class'] == 0][df['source'] == category]\n",
    "    chatgpt = df[df['class'] == 1][df['source'] == category]\n",
    "    \n",
    "    human_string = ' '.join(human['answer'].tolist())\n",
    "    chatgpt_string = ' '.join(chatgpt['answer'].tolist())\n",
    "    \n",
    "    human_split = human_string.split(\" \")\n",
    "    chatgpt_split = chatgpt_string.split(\" \")\n",
    "\n",
    "    human_list = [human_split[i:i + 200] for i in range(0, len(human_split), 200)]\n",
    "    chatgpt_list = [chatgpt_split[i:i + 200] for i in range(0, len(chatgpt_split), 200)]\n",
    "    \n",
    "    human_joined = []\n",
    "    for answer in human_list:\n",
    "        new_list = []\n",
    "        new_list.append(' '.join(answer))\n",
    "        human_joined.append(new_list)\n",
    "\n",
    "    chatgpt_joined = []\n",
    "    for answer in chatgpt_list:\n",
    "        new_list = []\n",
    "        new_list.append(' '.join(answer))\n",
    "        chatgpt_joined.append(new_list)\n",
    "    \n",
    "    human_df = pd.DataFrame({'answer' : human_joined, 'class' : 0, 'source': category})\n",
    "    chatgpt_df = pd.DataFrame({'answer' : chatgpt_joined, 'class' : 1, 'source': category})\n",
    "\n",
    "    human_df['answer'] = human_df['answer'].map(lambda x: ''.join(x))\n",
    "    chatgpt_df['answer'] = chatgpt_df['answer'].map(lambda x: ''.join(x))\n",
    "    \n",
    "    return human_df, chatgpt_df\n",
    "\n",
    "# Join data based on source\n",
    "reddit_eli5 = data_source(hc3_test[['answer', 'class', 'source']], 'reddit_eli5')\n",
    "finance = data_source(hc3_test[['answer', 'class', 'source']], 'finance')\n",
    "medicine = data_source(hc3_test[['answer', 'class', 'source']], 'medicine')\n",
    "wiki_csai = data_source(hc3_test[['answer', 'class', 'source']], 'wiki_csai')\n",
    "open_qa = data_source(hc3_test[['answer', 'class', 'source']], 'open_qa')\n",
    "\n",
    "# Create function\n",
    "def LD_mean(human, chatgpt):\n",
    "        ttr_human = human['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "        yulek_human = human['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "        mtld_human = human['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))\n",
    "    \n",
    "        ttr_chat_gpt = chatgpt['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "        yulek_chat_gpt = chatgpt['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "        mtld_chat_gpt = chatgpt['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))\n",
    "    \n",
    "        human_chatgpt = [\"Human\", \"ChatGPT\"]\n",
    "        metrics = [\"TTR\", \"Yule's K\", \"MTLD\"]\n",
    "        data = [[ttr_human.mean(), ttr_chat_gpt.mean()],\n",
    "                [yulek_human.mean(), yulek_chat_gpt.mean()],\n",
    "                [mtld_human.mean(), mtld_chat_gpt.mean()]]\n",
    "        print(pd.DataFrame(data, metrics, human_chatgpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Human     ChatGPT\n",
      "TTR         0.609441    0.538726\n",
      "Yule's K  110.853722  142.349614\n",
      "MTLD       86.051112   63.413385\n"
     ]
    }
   ],
   "source": [
    "LD_mean(reddit_eli5[0], reddit_eli5[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Human     ChatGPT\n",
      "TTR         0.670314    0.503938\n",
      "Yule's K  124.889604  184.901221\n",
      "MTLD       98.326788   56.632939\n"
     ]
    }
   ],
   "source": [
    "LD_mean(open_qa[0], open_qa[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Human     ChatGPT\n",
      "TTR         0.608622    0.542230\n",
      "Yule's K  127.085706  157.635022\n",
      "MTLD       76.286572   59.335829\n"
     ]
    }
   ],
   "source": [
    "LD_mean(wiki_csai[0], wiki_csai[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Human     ChatGPT\n",
      "TTR         0.605182    0.524778\n",
      "Yule's K  111.524756  154.899536\n",
      "MTLD       84.418261   60.852995\n"
     ]
    }
   ],
   "source": [
    "LD_mean(finance[0], finance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Human     ChatGPT\n",
      "TTR         0.659574    0.554276\n",
      "Yule's K   77.129425  124.041407\n",
      "MTLD      119.287712   71.274707\n"
     ]
    }
   ],
   "source": [
    "LD_mean(medicine[0], medicine[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
