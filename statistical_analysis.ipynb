{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from lexicalrichness import LexicalRichness # Lexical Analysis\n",
    "import scipy.stats as stats # Statistical Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24322, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path_or_buf=\"all.jsonl\", lines=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    '''This function is specifically designed for the HC3 dataset and the preprocessing thereof. \n",
    "    It mostly joins the data, deletes empty rows and duplicate rows.'''\n",
    "    \n",
    "    # Join all data, making it a string instead of a list\n",
    "    df['chatgpt_answers'] = df['chatgpt_answers'].map(lambda x: ''.join(x))\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: ''.join(x))\n",
    "\n",
    "    # Deleting empty rows\n",
    "    df = df.drop(df[df['chatgpt_answers'].map(lambda x: len(x) == 0)].index)\n",
    "    df = df.drop(df[df['human_answers'].map(lambda x: len(x) == 0)].index)\n",
    "\n",
    "    # Deleting duplicate rows\n",
    "    # Answering the same question will result in the same answers\n",
    "    # Therefore focusing on only question column\n",
    "    df = df.drop_duplicates(subset = ['question'], ignore_index = True)\n",
    "    df = df.drop_duplicates(subset = ['human_answers'], ignore_index = True)\n",
    "    df = df.drop_duplicates(subset = ['chatgpt_answers'], ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punc(df):\n",
    "    '''Function to remove punctuation and make everything lowercase.'''\n",
    "        \n",
    "    # Remove punctuation in ChatGPT answers column\n",
    "    df['chatgpt_answers'] = df['chatgpt_answers'].map(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    \n",
    "    # Remove punctuation in human answers column\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: re.sub(r'\\s(\\'\\S)', r'\\1', x))\n",
    "    df['human_answers'] = df['human_answers'].map(lambda x: re.sub(r'[^\\w\\s]', ' ', x.lower()))\n",
    "    \n",
    "    # Replace double spaces\n",
    "    df['human_answers'].replace(\"  \", \" \", regex = True, inplace = True)\n",
    "    df['human_answers'].replace(\"   \", \" \", regex = True, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def label_data(df):\n",
    "    '''This functions gives the each answer a label (0 for human answers, 1 for ChatGPT answers).'''\n",
    "    \n",
    "    # Labeling human answers\n",
    "    human = df[['question', 'human_answers', 'source']]\n",
    "    human['type'] = \"human\"\n",
    "    human.rename(columns = {'human_answers':'answer'}, inplace = True)\n",
    "    human['class'] = 0\n",
    "    \n",
    "    # Labeling ChatGPT answers\n",
    "    chatgpt = df[['question', 'chatgpt_answers', 'source']]\n",
    "    chatgpt['type'] = \"chatgpt\"\n",
    "    chatgpt.rename(columns = {'chatgpt_answers':'answer'}, inplace = True)\n",
    "    chatgpt['class'] = 1\n",
    "    \n",
    "    # Shuffle dataset and reset index\n",
    "    df = pd.concat([human, chatgpt], ignore_index = True)\n",
    "    df = shuffle(df)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def data_cleaning(df):\n",
    "    '''This function replaces some of the HC3-specific messiness, mostly in human answers.'''\n",
    "    \n",
    "    df['answer'].replace(\" n t \", \"nt \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" nt \", \"nt \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" s \", \"s \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" ve \", \"ve \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" re \", \"re \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"i m \", \"im \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" ll \", \"ll \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" t \", \"t \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" i d  \", \" id \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\" e  \", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"thf_media\", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"url_32\", \" \", regex = True, inplace = True)\n",
    "    df['answer'].replace(\"url_40\", \" \", regex = True, inplace = True)\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\r\\n', ' ', x))\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\r\\n\\r', ' ', x))\n",
    "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\n\\n', ' ', x))\n",
    "    df['answer'].replace(\"  \", \" \", regex = True, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc3_test = data_cleaning(label_data(remove_punc(data_preprocessing(df))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make evenly sized chunks (200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the data into human and chatgpt\n",
    "hc3_human = hc3_test[hc3_test['class'] == 0]\n",
    "hc3_chatgpt = hc3_test[hc3_test['class'] == 1]\n",
    "\n",
    "# Join the data and make it into a list format\n",
    "human_string = ' '.join(hc3_human['answer'].tolist())\n",
    "chatgpt_string = ' '.join(hc3_chatgpt['answer'].tolist())\n",
    "\n",
    "# Split data into words\n",
    "# That way I always have full words\n",
    "human_split = human_string.split(\" \")\n",
    "chatgpt_split = chatgpt_string.split(\" \")\n",
    "\n",
    "# Make evenly sized chunks: 200 words per answer\n",
    "human_list = [human_split[i:i + 200] for i in range(0, len(human_split), 200)]\n",
    "chatgpt_list = [chatgpt_split[i:i + 200] for i in range(0, len(chatgpt_split), 200)]\n",
    "\n",
    "# The chunks are still split\n",
    "# Joining the evenly sized chunks into one \"sentence\"\n",
    "human_joined = []\n",
    "for answer in human_list:\n",
    "    new_list = []\n",
    "    new_list.append(' '.join(answer))\n",
    "    human_joined.append(new_list)\n",
    "\n",
    "chatgpt_joined = []\n",
    "for answer in chatgpt_list:\n",
    "    new_list = []\n",
    "    new_list.append(' '.join(answer))\n",
    "    chatgpt_joined.append(new_list)\n",
    "    \n",
    "# Put both classes into a new Dataframe and concat them together\n",
    "human_df = pd.DataFrame({'answer' : human_joined, 'class' : 0})\n",
    "chatgpt_df = pd.DataFrame({'answer' : chatgpt_joined, 'class' : 1})\n",
    "    \n",
    "# Again, joining the answers, because they were still in a list format\n",
    "human_df['answer'] = human_df['answer'].map(lambda x: ''.join(x))\n",
    "chatgpt_df['answer'] = chatgpt_df['answer'].map(lambda x: ''.join(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df['ttr'] = human_df['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "human_df['yulek'] = human_df['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "human_df['mtld'] = human_df['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_df['ttr'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).ttr)\n",
    "chatgpt_df['yulek'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).yulek)\n",
    "chatgpt_df['mtld'] = chatgpt_df['answer'].map(lambda x: LexicalRichness(x).mtld(threshold = 0.72))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154.48248841822172, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat, p_value = stats.ttest_ind(a = human_df['ttr'], b = chatgpt_df['ttr'], alternative = 'two-sided', equal_var = False)\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-97.85798160612441, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat, p_value = stats.ttest_ind(a = human_df['yulek'], b = chatgpt_df['yulek'], alternative = 'two-sided', equal_var = False)\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116.2379270954642, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat, p_value = stats.ttest_ind(a = human_df['mtld'], b = chatgpt_df['mtld'], alternative = 'two-sided', equal_var = False)\n",
    "t_stat, p_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
